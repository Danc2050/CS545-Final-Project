{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test(5841 Good and 1659 Bad): (7500, 25)\n",
      "Train Good: (17523, 25)\n",
      "Train Bad: (4977, 25)\n",
      "Prior of the bad credit score: 0.2212. Prior of the good: 0.7788.\n",
      "Daniel is not a credit delinquent.\n",
      "[[5831   10]\n",
      " [1636   23]]\n",
      "[[5831   10]\n",
      " [1636   23]]\n",
      "Tp, fp, fn, tn:  5831 10 1636 23\n",
      "Accuracy: 78.05333333333333\n",
      "Precision: 0.9982879643896593\n",
      "Recall: 0.780902638275077\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training Naive Bayes Classifier for our final project CS545 taught by Anthony Rhodes\n",
    "Daniel Connelly, Dalton Boehnig, Ebele Esimai, Dawei Zhang, Daniel Lee\n",
    "Fall 2019\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sys\n",
    "from time import sleep\n",
    "\n",
    "def prior_prob(good, bad):\n",
    "    '''\n",
    "    Compute every positive and negative (1 and 0) in our data set. Since our categories are binary, we find out our prior probability by the following formula: class / total sum of classes\n",
    "    '''\n",
    "    positive = negative = total = 0\n",
    "\n",
    "    for i in range(np.shape(bad)[0]):\n",
    "        if bad[i][24] == 1.0: positive += 1; total +=1;\n",
    "\n",
    "    for i in range(np.shape(good)[0]):\n",
    "        if good[i][24] == 0.0: negative += 1; total +=1;\n",
    "\n",
    "    return np.divide(positive, total, dtype=np.float128), np.divide(negative, total, dtype=np.float128)\n",
    "\n",
    "def prob_model(features): return np.mean(features, axis=0, dtype=np.float128), np.std(features, axis=0, dtype=np.float128)\n",
    "\n",
    "def nb(mean, std_dev, feature):\n",
    "    '''one liner naive bayes for each feature in the feature set.\n",
    "       A mean is the mean of that class. A std_dev is the std_dev of that class. A feature is x_i.\n",
    "       Using np functions are important as they allow for greater precision (128)\n",
    "       Note: The dtype=np.float128 is necessary, but not to the extent I have done so (i.e., not every function needs it).\n",
    "    '''\n",
    "    return np.multiply(np.divide(1,((np.multiply(np.sqrt(np.multiply(2,np.pi, dtype=np.float128), dtype=np.float128),std_dev, dtype=np.float128))), dtype=np.float128),np.exp(-(((np.power(feature-mean, 2, dtype=np.float128)))/(np.multiply(2, (np.power(std_dev,2,dtype=np.float128))))), dtype=np.float128), dtype=np.float128)\n",
    "\n",
    "def class_NB(mean, stddev, test, prior):\n",
    "    class_type = [] # could be a bad credit person or good credit person. This function is generic, so we leave the name of this generic\n",
    "    i,j = test.shape\n",
    "    # for each example, compute the NB of the likelihood that it is POS (bad credit person)\n",
    "    for a in range(0,i):\n",
    "        feature = test[a][:-1] # we must not calculate the later column (the indicator of bad credit or good credit)\n",
    "        summation = 0\n",
    "        for b in range(0,j-1): # '     '\n",
    "            nb_result = nb(mean[b], stddev[b], feature[b])\n",
    "            if nb_result == 0:\n",
    "                nb_result = 1\n",
    "            summation += np.log10(nb_result) + np.log10(prior)\n",
    "        class_type.append(summation)\n",
    "    return class_type\n",
    "\n",
    "def conf_matrix(test, class_choice):\n",
    "    target_list = [];\n",
    "    for i in range(np.shape(test)[0]):\n",
    "        target_list.append(test[i][24])\n",
    "\n",
    "    # TODO -- is this function giving a wrong result?\n",
    "    print(confusion_matrix(target_list,class_choice)) # similar to program #1\n",
    "    conf_matrix = confusion_matrix(target_list,class_choice) # similar to program #1\n",
    "    tp = conf_matrix[0][0]\n",
    "    fp = conf_matrix[0][1]\n",
    "    fn = conf_matrix[1][0]\n",
    "    tn = conf_matrix[1][1]\n",
    "\n",
    "    print(conf_matrix)\n",
    "    print(\"Tp, fp, fn, tn: \",tp,fp,fn,tn)\n",
    "    print(\"Accuracy: {}\\nPrecision: {}\\nRecall: {}\".format((tp+tn)/7500 * 100, tp / (tp+fp),tp/(tp+fn)))\n",
    "\n",
    "def main():\n",
    "    # Set to true to read in credit data.\n",
    "    optional = True\n",
    "\n",
    "    # part 1 -- reading in data\n",
    "    import preprocess\n",
    "    examples, bad, good, test = preprocess.preproc()\n",
    "\n",
    "    # to print out shapes of our data to verify ratios\n",
    "    print(\"Test(5841 Good and 1659 Bad): \" + str(np.shape(test)))\n",
    "    print(\"Train Good: \" + str(np.shape(good)))\n",
    "    print(\"Train Bad: \" + str(np.shape(bad)))\n",
    "\n",
    "    #[] part 2 -- create probabilistic model\n",
    "    min_std_dev = 0.01\n",
    "\n",
    "    bad_prior, good_prior = prior_prob(good,bad) # note: last column does not affect calculations\n",
    "    print(\"Prior of the bad credit score: {}. Prior of the good: {}.\".format(bad_prior, good_prior))\n",
    "\n",
    "    #*- mean and std dev (of train set) for each class (bad credit,good credit)\n",
    "    bad_credit_mean, bad_stddev = prob_model(bad)\n",
    "    good_credit_mean, good_stddev = prob_model(good)\n",
    "\n",
    "    bad_stddev = np.clip(bad_stddev, min_std_dev, None) # enforces minimum std_devs to be > 0.\n",
    "    good_stddev = np.clip(good_stddev, min_std_dev, None) # '                                 '\n",
    "\n",
    "    # OPTIONAL STEP 1 -- Load in our own data\n",
    "    if optional:\n",
    "        daniel = np.genfromtxt(\"data/Daniel_Connelly_row.txt\", delimiter=',')\n",
    "        examples = np.append(examples, daniel)\n",
    "\n",
    "    #[] part 3 -- Run NB on test data -- gives P(x_i | class) for each class it is given\n",
    "    class_bad = class_NB(bad_credit_mean, bad_stddev, test, bad_prior)\n",
    "    class_good = class_NB(good_credit_mean, good_stddev, test, good_prior)\n",
    "\n",
    "    # OPTIONAL STEP 2 -- result data for Daniel\n",
    "    if optional:\n",
    "        if class_bad[7499] > class_good[7499]: print (\"Daniel is a credit delinquent.\")\n",
    "        else: print(\"Daniel is not a credit delinquent.\")\n",
    "\n",
    "    #* find argmax of decisions (bad credit or good credit) for each class of test set. One loop due to even # of data\n",
    "    class_choice = []\n",
    "    for i in range(0, len(class_bad)):\n",
    "        if class_bad[i] > class_good[i]: class_choice.append(1.0)\n",
    "        if class_bad[i] <= class_good[i]: class_choice.append(0.0)\n",
    "\n",
    "    #* Results\n",
    "    conf_matrix(test, class_choice)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
